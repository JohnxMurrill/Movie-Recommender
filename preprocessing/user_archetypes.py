"""
Special preprocessing for generating user archetypes. This script is not run as part of the normal 
data preprocessing for training and is instead used to generate representative user archetypes to 
improve the model's responsiveness to user queries. 

The archetypes are generated by using a k-means clustering on the user data and then selecting the centroids 
of the clusters as the archetypes. The archetypes are then used to generate a set of representative user query 
responses that can be used to give initial feedback while the model generates user specific responses.
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from joblib import dump

import logging
import numpy as np
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("movie_recommendation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('movie_recommendation')

users = pd.read_csv('../pre_processed_users.csv')

genre_rating_cols = [col for col in users.columns if '_avg_rating' in col]

# Convert genre specific ratings to "has watched" flags. Need to fill NaNs for K-means
for col in genre_rating_cols:
    # Create a new flag column, use same col name to replace
    flag_col = col.replace('_avg_rating', '_avg_rating')
    # Set 1 if rating exists and is > 0, else 0
    users[flag_col] = (users[col] > 0).astype(int)
    user_features_df = users.drop(columns=[col])

print(users.isna().sum())

# 1. Remove identifiers and redundant columns
logger.info("Removing identifiers and redundant columns...")
features_to_use = [col for col in user_features_df.columns 
                  if 'id' not in col.lower() and 'user' not in col.lower()]

# 2. Reduce redundancy in temporal features
# Keep only one time-related column (choose the most informative)
logger.info("Reducing redundancy in temporal features...")
time_columns = [col for col in features_to_use if 'year_' in col]
if len(time_columns) > 1:
    # Keep only mean, remove others
    columns_to_remove = [col for col in time_columns if 'mean' not in col]
    features_to_use = [col for col in features_to_use if col not in columns_to_remove]

clustering_features = user_features_df[features_to_use]


# Scale/normalize features first
scaler = StandardScaler()
scaled_features = scaler.fit_transform(clustering_features)

# Determine optimal k using elbow method
scores = []
# currently set to 25, but can be changed to test different ranges
# k_range = range(25, 251, 25)  # Try 25, 50, 75, ..., 250
k_range = range(25, 51, 25)
logger.info("Determining optimal number of clusters using silhouette score...")
for k in k_range:
    logger.info(f"Trying k={k}")
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    score = silhouette_score(scaled_features, kmeans.labels_)
    logger.info(f"Silhouette score for k={k}: {score}")
    scores.append(score)

# Choose k where silhouette score peaks
optimal_k = k_range[np.argmax(scores)]
logger.info(f"Optimal number of clusters: {optimal_k}")


# Run with optimal clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cluster_labels = kmeans.fit_predict(scaled_features)

# Get centroids - these are your archetypes!
archetypes = kmeans.cluster_centers_

# Unscale them back to original feature space for interpretability
archetype_profiles = scaler.inverse_transform(archetypes)

# Convert to DataFrame with meaningful column names
archetype_df = pd.DataFrame(
    archetype_profiles,
    columns=clustering_features.columns
)

# Analyze clusters to understand user segments
for i, archetype in enumerate(archetype_df.values):
    # Count users in this cluster
    cluster_size = np.sum(cluster_labels == i)
    logger.info(f"Cluster {i}: {cluster_size} users ({cluster_size/len(cluster_labels):.1%})")
    
    # Print top features for this archetype
    top_features = sorted(
        zip(users.columns, archetype),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:5]
    
    logger.info(f"  Key characteristics: {top_features}")

# Save archetypes and models
logger.info("Saving archetypes and models...")
dump({
    'archetypes': archetype_df,
    'scaler': scaler,
    'kmeans': kmeans
}, 'user_archetypes.joblib')